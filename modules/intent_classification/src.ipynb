{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Data/arabic_main.csv',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "# def ngram(token, n=3):\n",
    "#     output = []\n",
    "#     for i in range(n-1, len(token)):\n",
    "#         ngram = ' '.join(token[i-n+1:i+1])\n",
    "#         output.append(ngram)\n",
    "#     return output\n",
    "# def create_feature(text, nrange=(1, 4)):\n",
    "#     text_features = []\n",
    "#     text = text.lower()\n",
    "#     for i in range(nrange[0], nrange[1]+1):\n",
    "#         text_features += ngram(text.split(), i)\n",
    "#     return Counter(text_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "text = list(data['text'])#.apply(func=create_feature))\n",
    "label_list = list(data['intent'])\n",
    "for i in range(len(text)):\n",
    "    if text[i] is np.nan:\n",
    "        text[i] = \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(text, label_list, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer = tf_idf.TF_IDF(ngram=3,max_df=0.3,min_df=2)\n",
    "# X_train = vectorizer.fit_transform(text_list=X_train)\n",
    "# X_test = vectorizer.transform(text_list=X_test)\n",
    "\n",
    "#vectorizer = TfidfVectorizer(encoding='utf-8',ngram_range=(1,3),max_df=0.3,min_df=2)\n",
    "vectorizer = TfidfVectorizer(encoding='utf-8')\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)\n",
    "# from sklearn.feature_extraction import DictVectorizer\n",
    "# vectorizer = DictVectorizer(sparse = True)\n",
    "# X_train = vectorizer.fit_transform(X_train)\n",
    "# X_test = vectorizer.transform(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=1000, n_jobs=-1, random_state=0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#text_classifier = RandomForestClassifier(n_jobs=2, random_state=0)\n",
    "#text_classifier = LinearSVC(random_state=0,C=0.2)\n",
    "text_classifier = LogisticRegression(random_state=0,n_jobs=-1,max_iter=1000)\n",
    "text_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9433051869722557\n"
     ]
    }
   ],
   "source": [
    "predictions = text_classifier.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=1000, n_jobs=-1, random_state=0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = vectorizer.transform(text)\n",
    "text_classifier.fit(text, label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9650180940892642\n"
     ]
    }
   ],
   "source": [
    "predictions = text_classifier.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "general\n",
      "weather\n",
      "general\n",
      "general\n",
      "general\n",
      "general\n",
      "recommendation\n",
      "schedule\n",
      "general\n",
      "general\n"
     ]
    }
   ],
   "source": [
    "text_list = [\"عندى ميعاد مع الدكتور بكرة\",\"الجو عامل اية ؟\",\"من حصل على جائزة نوبل ؟\",\"متى يبدأ الدورى الإنجليزى ؟\",\"عندى معاد بكرة مع الدكتورابقى فكرنى\",\"اية احسن مطاعم فى مصر الجديدة\",\"عايز فيلم اكشن\",\"عندى ميتنج بكرة الساعة 5\",\"الجو ايه النهاردة ؟\",\"عايز مطاعم فالمعادى\"]\n",
    "for text in text_list:\n",
    "    text0 = vectorizer.transform([text])\n",
    "    intent = text_classifier.predict(text0)[0]\n",
    "    print(intent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
