{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvtXIYzZGq7s",
        "outputId": "1ce7f02c-00c8-4497-c059-bffc4120840c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFJG2ScLGv0g",
        "outputId": "35b61a1e-72ad-4c6b-8812-b2d9671600f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 776.7 MB 4.4 kB/s \n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.12.0+cu113 requires torch==1.11.0, but you have torch 1.7.0 which is incompatible.\n",
            "torchtext 0.12.0 requires torch==1.11.0, but you have torch 1.7.0 which is incompatible.\n",
            "torchaudio 0.11.0+cu113 requires torch==1.11.0, but you have torch 1.7.0 which is incompatible.\u001b[0m\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 8.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 74.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 42.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 880 kB 78.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 101 kB 13.4 MB/s \n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 298 kB 7.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 79.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 212 kB 95.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 140 kB 80.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 94 kB 3.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 144 kB 76.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 271 kB 60.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 7.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 6.7 MB/s \n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.2+zzzcolab20220527125636 requires tensorboard<2.9,>=2.8, but you have tensorboard 2.7.0 which is incompatible.\u001b[0m\n",
            "\u001b[K     |████████████████████████████████| 126 kB 7.1 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install torch==1.7.0 --quiet\n",
        "!pip install transformers==4.12.5 --quiet\n",
        "!pip install datasets==1.16.1 --quiet\n",
        "!pip install sentencepiece==0.1.96 --quiet\n",
        "!pip install tensorboard==2.7.0 --quiet\n",
        "!pip install PyArabic --quiet\n",
        "!pip install farasapy --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNEYUd3EJaLd",
        "outputId": "83aa93a2-354c-43dc-902f-2ddaa5339216"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'arabert'...\n",
            "remote: Enumerating objects: 569, done.\u001b[K\n",
            "remote: Counting objects: 100% (34/34), done.\u001b[K\n",
            "remote: Compressing objects: 100% (11/11), done.\u001b[K\n",
            "remote: Total 569 (delta 26), reused 24 (delta 23), pack-reused 535\u001b[K\n",
            "Receiving objects: 100% (569/569), 9.12 MiB | 11.89 MiB/s, done.\n",
            "Resolving deltas: 100% (327/327), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/aub-mind/arabert/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "wlFjp3vgGxSd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "from transformers import GPT2TokenizerFast\n",
        "from arabert.preprocess import ArabertPreprocessor\n",
        "\n",
        "# MODEL_NAME = 'aubmindlab/aragpt2-base'\n",
        "MODEL_NAME = '/content/drive/MyDrive/run_clm_hageen'\n",
        "DATA_PATH = '/content/drive/MyDrive/GP/gpt2/data'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "id": "3Ad1-_yGGzOM",
        "outputId": "2f346b36-52fd-4cee-9de7-8407ae1da806"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Model provided is not in the accepted model list. Preprocessor will default to a base Arabic preprocessor\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "HTTPError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-4e7f57367223>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0marabert_prep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mArabertPreprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMODEL_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPT2TokenizerFast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1653\u001b[0m                 \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1654\u001b[0m                 \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1655\u001b[0;31m                 \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_files_only\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1656\u001b[0m             )\n\u001b[1;32m   1657\u001b[0m             additional_files_names = {\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mget_fast_tokenizer_file\u001b[0;34m(path_or_repo, revision, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m   3466\u001b[0m     \u001b[0;31m# Inspect all files from the repo/folder.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3467\u001b[0m     all_files = get_list_of_files(\n\u001b[0;32m-> 3468\u001b[0;31m         \u001b[0mpath_or_repo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_files_only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3469\u001b[0m     )\n\u001b[1;32m   3470\u001b[0m     \u001b[0mtokenizer_files_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mget_list_of_files\u001b[0;34m(path_or_repo, revision, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m   1817\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1818\u001b[0m     model_info = HfApi(endpoint=HUGGINGFACE_CO_RESOLVE_ENDPOINT).model_info(\n\u001b[0;32m-> 1819\u001b[0;31m         \u001b[0mpath_or_repo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1820\u001b[0m     )\n\u001b[1;32m   1821\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrfilename\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msiblings\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/huggingface_hub/hf_api.py\u001b[0m in \u001b[0;36mmodel_info\u001b[0;34m(self, repo_id, revision, token, timeout, securityStatus)\u001b[0m\n\u001b[1;32m   1134\u001b[0m             \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstatus_query_param\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         )\n\u001b[0;32m-> 1136\u001b[0;31m         \u001b[0m_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1137\u001b[0m         \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mModelInfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/huggingface_hub/utils/_errors.py\u001b[0m in \u001b[0;36m_raise_for_status\u001b[0;34m(request)\u001b[0m\n\u001b[1;32m     82\u001b[0m         )\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0m_raise_with_request_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/huggingface_hub/utils/_errors.py\u001b[0m in \u001b[0;36m_raise_with_request_id\u001b[0;34m(request)\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34mf\" (Request ID: {request_id})\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/huggingface_hub/utils/_errors.py\u001b[0m in \u001b[0;36m_raise_with_request_id\u001b[0;34m(request)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0mrequest_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"X-Request-Id\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrequest_id\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    940\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 941\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    942\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    943\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://huggingface.co/api/models//content/drive/MyDrive/run_clm_hageen (Request ID: 8hkwyaxl6SKmwSQCW2E34)"
          ]
        }
      ],
      "source": [
        "arabert_prep = ArabertPreprocessor(model_name=MODEL_NAME)\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(MODEL_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OGPDYo8YH8be",
        "outputId": "4f58c6ec-606f-4eac-84f3-93e46421e7c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading the valid utters...\n",
            "Tokenize the valid utters...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9287/9287 [00:17<00:00, 542.76it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved the valid ids.\n",
            "Loading the train utters...\n",
            "Tokenize the train utters...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 52613/52613 [01:33<00:00, 560.05it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved the train ids.\n"
          ]
        }
      ],
      "source": [
        "def tokenize_arabic_data(prefix):\n",
        "  print(f\"Loading the {prefix} utters...\")\n",
        "\n",
        "  dials = []\n",
        "  with open(f\"{DATA_PATH}/{prefix}_utters.json\", 'r') as f:\n",
        "    dials = json.load(f)\n",
        "  \n",
        "  print(f\"Tokenize the {prefix} utters...\")\n",
        "  ids = []\n",
        "  for dial in tqdm(dials):\n",
        "    dial_ids = []\n",
        "    for utter in dial:\n",
        "      proecessed_utter = arabert_prep.preprocess(utter)\n",
        "      tokens = tokenizer.tokenize(proecessed_utter)\n",
        "      token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "      dial_ids.append(token_ids)\n",
        "    ids.append(dial_ids)\n",
        "  \n",
        "  assert len(ids) == len(dials)\n",
        "  with open(f\"{DATA_PATH}/{prefix}_ids.json\", 'w') as f:\n",
        "    json.dump(ids, f)\n",
        "  print(f\"Saved the {prefix} ids.\")\n",
        "\n",
        "tokenize_arabic_data('valid')\n",
        "tokenize_arabic_data('train')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9z7-T-PcX1bc",
        "outputId": "09611bb7-7aee-4b49-a566-c6294b5ede4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘/content/drive/MyDrive/GP/gpt2/data/aubmindlab/aragpt2-base’: File exists\n"
          ]
        }
      ],
      "source": [
        "!mkdir /content/drive/MyDrive/GP/gpt2/data/aubmindlab/aragpt2-base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fuuotL7FKtSU"
      },
      "outputs": [],
      "source": [
        "# !cp /content/drive/MyDrive/GP/gpt2/data/valid_utters.json /content/drive/MyDrive/GP/gpt2/data/aubmindlab/aragpt2-base/valid_utters.json\n",
        "# !cp /content/drive/MyDrive/GP/gpt2/data/train_utters.json /content/drive/MyDrive/GP/gpt2/data/aubmindlab/aragpt2-base/train_utters.json\n",
        "# !cp /content/drive/MyDrive/GP/gpt2/data/train_ids.json /content/drive/MyDrive/GP/gpt2/data/aubmindlab/aragpt2-base/train_ids.json\n",
        "# !cp /content/drive/MyDrive/GP/gpt2/data/valid_ids.json /content/drive/MyDrive/GP/gpt2/data/aubmindlab/aragpt2-base/valid_ids.json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /content/arabert /content/src/"
      ],
      "metadata": {
        "id": "tCcWeI58esT_"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /content/drive/MyDrive/GP/gpt2/* /content/"
      ],
      "metadata": {
        "id": "5rM5sDFvb0ck"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3DzQKkNRW9p",
        "outputId": "960d3811-5e03-4ffd-9601-ab34cbbf9b24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading the tokenizer...\n",
            "WARNING:root:Model provided is not in the accepted model list. Preprocessor will default to a base Arabic preprocessor\n",
            "Loading the model...\n",
            "Loading the optimizer...\n",
            "Loading train & valid data...\n",
            "Loading train_id.json...\n",
            "/content/drive/MyDrive/GP/gpt2/data/aubmindlab/aragpt2-base/train_ids.json\n",
            "Processing train data...\n",
            "100% 52613/52613 [00:04<00:00, 10858.27it/s]\n",
            "Loading valid_id.json...\n",
            "/content/drive/MyDrive/GP/gpt2/data/aubmindlab/aragpt2-base/valid_ids.json\n",
            "Processing valid data...\n",
            "100% 9287/9287 [00:00<00:00, 15686.50it/s]\n",
            "Setting finished.\n",
            "Training starts.\n",
            "##################################################Epoch: 1##################################################\n",
            "100% 27790/27790 [1:08:46<00:00,  6.73it/s]\n",
            "Train loss: 3.860809689226147 || Train perplexity: 52.998595131280055\n",
            "Validation processing...\n",
            "100% 4961/4961 [03:07<00:00, 26.44it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "**********Current best checkpoint is saved.**********\n",
            "/content/drive/MyDrive/GP/diarzgpt/aubmindlab/aragpt2-base/best_ckpt_epoch=1_valid_loss=3.4544.ckpt\n",
            "Best valid loss: 3.4544105130538565\n",
            "Valid loss: 3.4544105130538565 || Valid perplexity: 35.42251695002599\n",
            "##################################################Epoch: 2##################################################\n",
            "100% 27790/27790 [1:08:41<00:00,  6.74it/s]\n",
            "Train loss: 3.494162922971066 || Train perplexity: 35.47254907342761\n",
            "Validation processing...\n",
            "100% 4961/4961 [03:07<00:00, 26.43it/s]\n",
            "**********Current best checkpoint is saved.**********\n",
            "/content/drive/MyDrive/GP/diarzgpt/aubmindlab/aragpt2-base/best_ckpt_epoch=2_valid_loss=3.3567.ckpt\n",
            "Best valid loss: 3.3566730644788936\n",
            "Valid loss: 3.3566730644788936 || Valid perplexity: 32.37246131844108\n",
            "##################################################Epoch: 3##################################################\n",
            "100% 27790/27790 [1:08:46<00:00,  6.73it/s]\n",
            "Train loss: 3.3339181581818402 || Train perplexity: 30.1007081220458\n",
            "Validation processing...\n",
            "100% 4961/4961 [03:07<00:00, 26.40it/s]\n",
            "**********Current best checkpoint is saved.**********\n",
            "/content/drive/MyDrive/GP/diarzgpt/aubmindlab/aragpt2-base/best_ckpt_epoch=3_valid_loss=3.3075.ckpt\n",
            "Best valid loss: 3.3074762631606442\n",
            "Valid loss: 3.3074762631606442 || Valid perplexity: 31.061728052976655\n",
            "##################################################Epoch: 4##################################################\n",
            "100% 27790/27790 [1:08:50<00:00,  6.73it/s]\n",
            "Train loss: 3.221864149728324 || Train perplexity: 26.844749382512525\n",
            "Validation processing...\n",
            "100% 4961/4961 [03:07<00:00, 26.40it/s]\n",
            "**********Current best checkpoint is saved.**********\n",
            "/content/drive/MyDrive/GP/diarzgpt/aubmindlab/aragpt2-base/best_ckpt_epoch=4_valid_loss=3.2736.ckpt\n",
            "Best valid loss: 3.2735527107201863\n",
            "Valid loss: 3.2735527107201863 || Valid perplexity: 30.24109434197204\n",
            "##################################################Epoch: 5##################################################\n",
            "100% 27790/27790 [1:08:48<00:00,  6.73it/s]\n",
            "Train loss: 3.139330250762615 || Train perplexity: 24.659043423268983\n",
            "Validation processing...\n",
            "100% 4961/4961 [03:08<00:00, 26.28it/s]\n",
            "**********Current best checkpoint is saved.**********\n",
            "/content/drive/MyDrive/GP/diarzgpt/aubmindlab/aragpt2-base/best_ckpt_epoch=5_valid_loss=3.253.ckpt\n",
            "Best valid loss: 3.2530158027553\n",
            "Valid loss: 3.2530158027553 || Valid perplexity: 29.847979729561477\n",
            "##################################################Epoch: 6##################################################\n",
            "100% 27790/27790 [1:08:48<00:00,  6.73it/s]\n",
            "Train loss: 3.078923885107641 || Train perplexity: 23.20288785476829\n",
            "Validation processing...\n",
            "100% 4961/4961 [03:07<00:00, 26.41it/s]\n",
            "**********Current best checkpoint is saved.**********\n",
            "/content/drive/MyDrive/GP/diarzgpt/aubmindlab/aragpt2-base/best_ckpt_epoch=6_valid_loss=3.2402.ckpt\n",
            "Best valid loss: 3.2402343625648444\n",
            "Valid loss: 3.2402343625648444 || Valid perplexity: 29.63856066342588\n",
            "##################################################Epoch: 7##################################################\n",
            "100% 27790/27790 [1:08:46<00:00,  6.73it/s]\n",
            "Train loss: 3.0341740855835204 || Train perplexity: 22.156559499672483\n",
            "Validation processing...\n",
            "100% 4961/4961 [03:07<00:00, 26.42it/s]\n",
            "**********Current best checkpoint is saved.**********\n",
            "/content/drive/MyDrive/GP/diarzgpt/aubmindlab/aragpt2-base/best_ckpt_epoch=7_valid_loss=3.2353.ckpt\n",
            "Best valid loss: 3.2353350316966356\n",
            "Valid loss: 3.2353350316966356 || Valid perplexity: 29.63323732004894\n",
            "##################################################Epoch: 8##################################################\n",
            "100% 27790/27790 [1:08:47<00:00,  6.73it/s]\n",
            "Train loss: 3.0056657482969458 || Train perplexity: 21.51279414390737\n",
            "Validation processing...\n",
            "100% 4961/4961 [03:07<00:00, 26.42it/s]\n",
            "**********Current best checkpoint is saved.**********\n",
            "/content/drive/MyDrive/GP/diarzgpt/aubmindlab/aragpt2-base/best_ckpt_epoch=8_valid_loss=3.2322.ckpt\n",
            "Best valid loss: 3.232212997232563\n",
            "Valid loss: 3.232212997232563 || Valid perplexity: 29.634787503713273\n",
            "##################################################Epoch: 9##################################################\n",
            "100% 27790/27790 [1:08:48<00:00,  6.73it/s]\n",
            "Train loss: 2.9883662392785113 || Train perplexity: 21.141128782953192\n",
            "Validation processing...\n",
            "100% 4961/4961 [03:08<00:00, 26.27it/s]\n",
            "Best valid loss: 3.232212997232563\n",
            "Valid loss: 3.2324086485479033 || Valid perplexity: 29.696544826763624\n",
            "##################################################Epoch: 10##################################################\n",
            "100% 27790/27790 [1:08:42<00:00,  6.74it/s]\n",
            "Train loss: 2.980801629541846 || Train perplexity: 20.970844570541175\n",
            "Validation processing...\n",
            "100% 4961/4961 [03:07<00:00, 26.42it/s]\n",
            "Best valid loss: 3.232212997232563\n",
            "Valid loss: 3.232851837544498 || Valid perplexity: 29.726136247325005\n",
            "Training finished!\n"
          ]
        }
      ],
      "source": [
        "!sh exec_train.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DqHMNgizIS9p",
        "outputId": "7aa6b937-69b4-4714-d3ca-c8016efba48f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading the tokenizer...\n",
            "WARNING:root:Model provided is not in the accepted model list. Preprocessor will default to a base Arabic preprocessor\n",
            "Loading the model...\n",
            "Loading the trained checkpoint...\n",
            "The inference will start with the specified checkpoint.\n",
            "Setting finished.\n",
            "Let's start!\n",
            "If you want to quit the conversation, please type \"Abort!\".\n",
            "You: ازيك\n",
            "Bot: هذا ليس جيدا\n",
            "You: عامل ايه\n",
            "Bot: أنت تعلم أن الألغام يتم زرعها في أعماق البحار\n",
            "You: حقا؟\n",
            "Bot: نعم ، أنها صغيرة جدا ، لذلك يمكن أن تكون جافة جدا إذا كان مجرد صبغت العلكة.\n",
            "You: هذا مدهش\n",
            "Bot: من اللطيف جدا منك أن تقول لي ما إذا كنت حقا تعرف أن الألغام يمكن أن تسبب صداعا كبيرا.\n",
            "You: انا لا اعرف هذا\n",
            "Bot: لكنني لا أعرف من أين أبدأ عندما أفكر في الحصول على الألغام.\n",
            "You: Abort!\n",
            "Bot: Good bye.\n"
          ]
        }
      ],
      "source": [
        "!sh exec_infer.sh"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "aragpt2--best",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}