{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "source": [
    "import requests\r\n",
    "from bs4 import BeautifulSoup\r\n",
    "from bs4 import NavigableString"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "source": [
    "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:96.0) Gecko/20100101 Firefox/96.0'}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "source": [
    "def scarp_novel_part(PATH, flag = ':'):\r\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:96.0) Gecko/20100101 Firefox/96.0'}\r\n",
    "    HOST = \"https://www.wattpad.com\"\r\n",
    "    \r\n",
    "    page = requests.get(HOST + PATH, headers=headers)\r\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\r\n",
    "\r\n",
    "    pre = soup.find(\"pre\")\r\n",
    "    ps = pre.find_all(\"p\")\r\n",
    "\r\n",
    "    dialog = []\r\n",
    "\r\n",
    "    for p in ps:\r\n",
    "        contents = p.contents\r\n",
    "\r\n",
    "        for text in contents:\r\n",
    "            if type(text) != NavigableString: continue # new-line\r\n",
    "            \r\n",
    "            if text.find(flag) == -1: continue\r\n",
    "\r\n",
    "            dialog.append( text[text.find(flag) + 1: ].strip() )\r\n",
    "\r\n",
    "    return dialog"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "source": [
    "def get_story_parts_links(URL):\r\n",
    "    page = requests.get(URL, headers=headers)\r\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\r\n",
    "\r\n",
    "    paths = []\r\n",
    "\r\n",
    "    res = soup.find_all(\"div\", {\"class\": \"story-parts\"})\r\n",
    "    res = res[0]\r\n",
    "\r\n",
    "    anchors = res.find_all(\"a\")\r\n",
    "\r\n",
    "    for a in anchors:\r\n",
    "        paths.append(a.attrs[\"href\"])\r\n",
    "\r\n",
    "    return paths"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "source": [
    "def scrap_whole_story(story_url, flag = \":\"):\r\n",
    "    paths = get_story_parts_links(story_url)\r\n",
    "\r\n",
    "    dialogs = []\r\n",
    "\r\n",
    "    for path in paths:\r\n",
    "        dialog = scarp_novel_part(path, flag)\r\n",
    "        dialogs.append(dialog)\r\n",
    "\r\n",
    "    return dialogs\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "source": [
    "# احببت كاره النساء\r\n",
    "URL_1 = \"https://www.wattpad.com/story/181957426-%D8%B1%D9%88%D8%A7%D9%8A%D8%A9-%D8%A7%D8%AD%D8%A8%D8%A8%D8%AA-%D9%83%D8%A7%D8%B1%D9%87-%D8%A7%D9%84%D9%86%D8%B3%D8%A7%D8%A1\"\r\n",
    "res_1 = scrap_whole_story(URL_1, \":\")\r\n",
    "print(sum([len(x) for x in res_1]))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2517\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "source": [
    "# ضائعة في قلب ميت\r\n",
    "URL_2 = \"https://www.wattpad.com/story/115859904-%D8%B6%D8%A7%D8%A6%D8%B9%D9%87-%D9%81%D9%8A-%D9%82%D9%84%D8%A8-%D9%85%D9%8A%D8%AA\"\r\n",
    "res_2 = scrap_whole_story(URL_2, \":\")\r\n",
    "print(sum([len(x) for x in res_2]))\r\n",
    "\r\n",
    "with open(\"./scrapped-data/ضائعة فى قلب ميت.txt\", \"w\", encoding='utf-8') as f:\r\n",
    "    for x in res_2:\r\n",
    "        if len(x):\r\n",
    "            f.write(\"\\n\".join(x))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2230\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "source": [
    "# ضائعة في قلب ميت 2\r\n",
    "URL_3 = \"https://www.wattpad.com/story/115893456-%D8%B6%D8%A7%D8%A6%D8%B9%D9%87-%D9%81%D9%8A-%D9%82%D9%84%D8%A8-%D9%85%D9%8A%D8%AA-2\"\r\n",
    "res_3 = scrap_whole_story(URL_3, \":\")\r\n",
    "print(sum([len(x) for x in res_3]))\r\n",
    "\r\n",
    "with open(\"./scrapped-data/2 ضائعة فى قلب ميت.txt\", \"w\", encoding='utf-8') as f:\r\n",
    "    for x in res_3:\r\n",
    "        if len(x):\r\n",
    "            f.write(\"\\n\".join(x))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1266\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "source": [
    "# عيلة مصرية 2\r\n",
    "URL_4 = \"https://www.wattpad.com/story/151100959-%D8%B9%D9%8A%D9%84%D8%A9-%D9%85%D8%B5%D8%B1%D9%8A%D8%A9-2%2B\"\r\n",
    "res_4 = scrap_whole_story(URL_4, \".\")\r\n",
    "print(sum([len(x) for x in res_4]))\r\n",
    "\r\n",
    "with open(\"./scrapped-data/2 عيلة مصرية.txt\", \"w\", encoding='utf-8') as f:\r\n",
    "    for x in res_4:\r\n",
    "        if len(x):\r\n",
    "            f.write(\"\\n\".join(x))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1672\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "source": [
    "# قصر الدبلوماسى\r\n",
    "URL_5 = \"https://www.wattpad.com/story/12502070-%D9%82%D8%B5%D8%B1-%D8%A7%D9%84%D8%AF%D8%A8%D9%84%D9%88%D9%85%D8%A7%D8%B3%D9%89\"\r\n",
    "res_5 = scrap_whole_story(URL_5, \":\")\r\n",
    "print(sum([len(x) for x in res_5]))\r\n",
    "\r\n",
    "with open(\"./scrapped-data/قصر الدبلوماسى.txt\", \"w\", encoding='utf-8') as f:\r\n",
    "    for x in res_5:\r\n",
    "        if len(x):\r\n",
    "            f.write(\"\\n\".join(x))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1587\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "source": [
    "# الدّين\r\n",
    "URL_6 = \"https://www.wattpad.com/story/51207143-%D8%A7%D9%84%D9%80%D8%AF%D9%91%D9%8A%D9%92%D9%80%D9%86\"\r\n",
    "res_6 = scrap_whole_story(URL_6, \":\")\r\n",
    "print(sum([len(x) for x in res_6]))\r\n",
    "\r\n",
    "with open(\"./scrapped-data/الدّين.txt\", \"w\", encoding='utf-8') as f:\r\n",
    "    for x in res_6:\r\n",
    "        if len(x):\r\n",
    "            f.write(\"\\n\".join(x))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "640\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "source": [
    "# أبيض VS أحمر\r\n",
    "URL_7 = \"https://www.wattpad.com/story/47735132-%D8%A3%D8%AD%D9%85%D8%B1-vs-%D8%A3%D8%A8%D9%8A%D8%B6\"\r\n",
    "res_7 = scrap_whole_story(URL_7, \":\")\r\n",
    "print(sum([len(x) for x in res_7]))\r\n",
    "\r\n",
    "with open(\"./scrapped-data/أبيض وأحمر.txt\", \"w\", encoding='utf-8') as f:\r\n",
    "    for x in res_7:\r\n",
    "        if len(x):\r\n",
    "            f.write(\"\\n\".join(x))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "771\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "source": [
    "# count lines in all scrapped novels\r\n",
    "import os\r\n",
    "\r\n",
    "s = 0\r\n",
    "for path in os.listdir(\"./scrapped-data/\"):\r\n",
    "    with open(\"./scrapped-data/\" + path, \"r\", encoding=\"utf-8\") as f:\r\n",
    "        s += sum(1 for line in f)\r\n",
    "\r\n",
    "print(f\"Total lines: {s}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total lines: 10509\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "source": [
    "# def tmp_scarp_novel_part(PATH):\r\n",
    "#     headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:96.0) Gecko/20100101 Firefox/96.0'}\r\n",
    "#     HOST = \"https://www.wattpad.com\"\r\n",
    "\r\n",
    "#     page = requests.get(HOST + PATH, headers=headers)\r\n",
    "#     soup = BeautifulSoup(page.content, \"html.parser\")\r\n",
    "\r\n",
    "#     pre = soup.find(\"pre\")\r\n",
    "#     ps = pre.find_all(\"p\")\r\n",
    "\r\n",
    "#     dialog = []\r\n",
    "#     take_it = 0\r\n",
    "#     s = \"\"\r\n",
    "\r\n",
    "#     for i in range(len(ps)):\r\n",
    "#         for tag in ps[i].contents:\r\n",
    "            \r\n",
    "#             if take_it == 1:\r\n",
    "#                 if tag.contents == []:\r\n",
    "#                     take_it = 0\r\n",
    "#                     dialog.append(s.strip())\r\n",
    "#                     s = \"\"\r\n",
    "#                 else:\r\n",
    "#                     s += tag.text\r\n",
    "\r\n",
    "#             if tag.text.strip() == \":\":\r\n",
    "#                 take_it = 1\r\n",
    "#                 continue\r\n",
    "\r\n",
    "#     return dialog"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "source": [
    "# good_paths = [0, 1, 14]\r\n",
    "# links = get_story_parts_links(URL_6)\r\n",
    "# part = scarp_novel_part(links[2])\r\n",
    "# PATH = links[2]\r\n",
    "\r\n",
    "# tmp = []\r\n",
    "\r\n",
    "# for i in range(len(links)):\r\n",
    "#     if i not in good_paths:\r\n",
    "#         dialog = tmp_scarp_novel_part(links[i])\r\n",
    "#         tmp.append(dialog)\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.3",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "4905652b14e4b7eb92899b78ac499a22c488804455b27940a322fd82aaf71031"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}