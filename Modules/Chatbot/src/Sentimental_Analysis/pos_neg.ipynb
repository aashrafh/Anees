{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import preprocess\n",
    "import numpy as np\n",
    "import verb_extraction\n",
    "import stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = pd.read_csv('../../Data/Sentimental_Analysis/main_preprocessed.csv',encoding='utf-8')\n",
    "new_data = pd.read_csv('new/final.csv',encoding='utf-8')\n",
    "data = pd.concat([data,new_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"../../Data/stopwords.txt\",\"r\",encoding=\"utf-8\")\n",
    "stopwords = file.read().split()\n",
    "file.close()\n",
    "def get_tokens(text):\n",
    "    dictionary = {'د':\"دكتور\"}#to be continued\n",
    "    words_after_split = text.split()\n",
    "    for index , word in enumerate(words_after_split):\n",
    "        if word in dictionary.keys():\n",
    "            w = dictionary[word]\n",
    "            words_after_split[index] = w\n",
    "    new_words = list()\n",
    "    for word in words_after_split:\n",
    "        if word not in stopwords:\n",
    "            new_words.append(word)\n",
    "    return new_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TWEET</th>\n",
       "      <th>LABEL</th>\n",
       "      <th>pos_neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>شعر حزن شديد والاسف لكنني خرج قريبا</td>\n",
       "      <td>sadness</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ورق مصنوع ورق</td>\n",
       "      <td>sadness</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>مجرد شعور غريب اطوار والازرق</td>\n",
       "      <td>anger</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>حصول علاج</td>\n",
       "      <td>joy</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>شعر تقدير</td>\n",
       "      <td>joy</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 TWEET    LABEL pos_neg\n",
       "0  شعر حزن شديد والاسف لكنني خرج قريبا  sadness     neg\n",
       "1                        ورق مصنوع ورق  sadness     neg\n",
       "2         مجرد شعور غريب اطوار والازرق    anger     neg\n",
       "3                            حصول علاج      joy     pos\n",
       "4                            شعر تقدير      joy     pos"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = list(data['TWEET'])[10000:]\n",
    "label = list(data['pos_neg'])[10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(tweet, label, test_size=0.2, random_state=0)\n",
    "vectorizer = TfidfVectorizer(encoding='utf-8',ngram_range=(1,3),max_df=0.85,min_df=2)\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=0.2, random_state=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "#text_classifier = RandomForestClassifier(n_jobs=2, random_state=0)\n",
    "text_classifier = LinearSVC(random_state=0,C=0.2)\n",
    "text_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8095406360424028\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "predictions = text_classifier.predict(X_test)\n",
    "print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pos' 'neg' 'neg' 'pos']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text_list = [\"انا استمتعت جدا باليوم\",\"انا اكرهك\",\"انا اريد ان انتحر\",\"انا سعيد\"]\n",
    "for index in range(len(text_list)):\n",
    "    text_list[index] = preprocess.pre_process(text_list[index])\n",
    "    tokens = get_tokens(text_list[index])\n",
    "    tokens_verb_noun = verb_extraction.extract_stem_verb(tokens,{})\n",
    "    tokens_verb_noun = np.array(stemming.stem(tokens_verb_noun))\n",
    "    text_list[index] = ' '.join([str(elem) for elem,_ in tokens_verb_noun])\n",
    "for i in range(len(text_list)):\n",
    "    text_list[i] = re.sub(\"[a-zA-Z]\", \" \", text_list[i]) # remove english letters\n",
    "    text_list[i] = re.sub('\\n', ' ', text_list[i]) # remove \\n from text\n",
    "    text_list[i] = re.sub(r'\\d+', '', text_list[i]) #remove number\n",
    "    text_list[i] = re.sub(r'http\\S+', '', text_list[i]) # remove links\n",
    "    text_list[i] = re.sub(' +', ' ',text_list[i]) # remove extra space\n",
    "    text_list[i] = text_list[i].strip() #remove whitespaces\n",
    "tweet = vectorizer.transform(text_list)\n",
    "prediction = text_classifier.predict(tweet)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = f'../../utils/sentmental_+-_model.sav'\n",
    "pickle.dump(text_classifier, open(filename, 'wb'))\n",
    "filename = f'../../utils/tfidf_+-_model.sav'\n",
    "pickle.dump(vectorizer, open(filename, 'wb'))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
