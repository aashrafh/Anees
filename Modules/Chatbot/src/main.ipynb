{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ahmed Sherif\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator CountVectorizer from version 0.23.1 when using version 1.1.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ahmed Sherif\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator LinearSVC from version 0.23.1 when using version 1.1.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ahmed Sherif\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.23.1 when using version 1.1.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ahmed Sherif\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.0.2 when using version 1.1.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ahmed Sherif\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator TfidfTransformer from version 1.0.2 when using version 1.1.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ahmed Sherif\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 1.0.2 when using version 1.1.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "stopwords, ner_instance, verbs, nouns, emotions_model, emotions_tf_idf, intent_model, tokenizer, recomm_intent_model, recomm_tokenizer, location_recomm, movie_recomm,q_not_model , tf_idf_q_not = main.get_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------\n",
      "Input                          | محمد يذهب الى المدرسة كل يوم\n",
      "----------------------------------------------------------------------------------------\n",
      "Text after Preprocessing       | محمد يذهب الي المدرسه كل يوم\n",
      "Tokens                         | [محمد , يذهب , المدرسه , يوم ]\n",
      "NER                            | محمد B-PERS\n",
      "Text after  Stemming           | محمد ذهب مدرسه يوم\n",
      "POS                            \n",
      "[['محمد' 'B-PERS']\n",
      " ['ذهب' 'vPresent']\n",
      " ['مدرسه' 'n']\n",
      " ['يوم' 'n']]\n",
      "----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "input_text = \"محمد يذهب الى المدرسة كل يوم\"\n",
    "text, tokens, ents, tokens_verb_noun = main.NLU(input_text, stopwords, ner_instance, verbs, nouns)\n",
    "tokens_verb_noun = np.array(tokens_verb_noun)\n",
    "print(\"---------------------------------------------------------------------------------------\")\n",
    "print(\"Input                          |\"+\" \"+ input_text)\n",
    "print(\"----------------------------------------------------------------------------------------\")\n",
    "print(\"Text after Preprocessing       |\"+\" \"+ text)\n",
    "print(\"Tokens                         |\"+\" [\"+ \" , \".join(tokens)+\" ]\")\n",
    "print(\"NER                            |\"+\" \"+ str(\"\".join(ents.keys())) + \" \" + str(\"\".join(ents.values())))\n",
    "print(\"Text after  Stemming           |\"+\" \"+ \" \".join(tokens_verb_noun[:,0]))\n",
    "print(\"POS                            \")\n",
    "print(tokens_verb_noun)\n",
    "print(\"----------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------\n",
      "Input                          | احجزلى ميعاد مع دكتور الاسنان بكرة الساعة 4 ونص\n",
      "----------------------------------------------------------------------------------------\n",
      "Tokens                         | [احوال , الجو , يومين , الاقصر ]\n",
      "Now Time                       | [07/16/2022, 05:24:51 ]\n",
      "Edited Time                    | [07/18/2022, 05:24:00 ]\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "---------------------------------------------------------------------------------------\n",
      "Input                          | احجزلى ميعاد مع دكتور الاسنان بكرة الساعة 4 ونص\n",
      "----------------------------------------------------------------------------------------\n",
      "Tokens                         | [احوال , الجو , يومين , الاقصر ]\n",
      "Content                        | [احوال الجو ]\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "الفيوم\n",
      "---------------------------------------------------------------------------------------\n",
      "Input                          | ما احوال الجو بعد يومين فى الفيوم\n",
      "----------------------------------------------------------------------------------------\n",
      "Content                        | [28°C الجو سماء صافية درجة الحرارة  ]\n",
      "City Name                      | [الفيوم ]\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time_extract\n",
    "import content_extract\n",
    "from datetime import datetime\n",
    "input_text = \"احجزلى ميعاد مع دكتور الاسنان بكرة الساعة 4 ونص\"\n",
    "tokens_verb_noun = np.array(tokens_verb_noun)\n",
    "print(\"---------------------------------------------------------------------------------------\")\n",
    "print(\"Input                          |\"+\" \"+ input_text)\n",
    "print(\"----------------------------------------------------------------------------------------\")\n",
    "print(\"Tokens                         |\"+\" [\"+ \" , \".join(tokens)+\" ]\")\n",
    "edited_time, tokens_used, filtered_tokens = time_extract.main(tokens, tokens_verb_noun )\n",
    "print(\"Now Time                       |\"+\" [\"+ datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\") + \" ]\")\n",
    "print(\"Edited Time                    |\"+\" [\"+ edited_time.strftime(\"%m/%d/%Y, %H:%M:%S\") + \" ]\")\n",
    "print(\"----------------------------------------------------------------------------------------\\n\\n\\n\")\n",
    "\n",
    "\n",
    "print(\"---------------------------------------------------------------------------------------\")\n",
    "print(\"Input                          |\"+\" \"+ input_text)\n",
    "print(\"----------------------------------------------------------------------------------------\")\n",
    "print(\"Tokens                         |\"+\" [\"+ \" , \".join(tokens)+\" ]\")\n",
    "content = content_extract.get_schedule_content(text, tokens_used, filtered_tokens)\n",
    "print(\"Content                        |\"+\" [\"+ content + \" ]\")\n",
    "print(\"----------------------------------------------------------------------------------------\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Dont run this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "الفيوم\n",
      "---------------------------------------------------------------------------------------\n",
      "Input                          | ما احوال الجو بعد يومين فى الفيوم\n",
      "----------------------------------------------------------------------------------------\n",
      "Content                        | [28°C الجو سماء صافية درجة الحرارة  ]\n",
      "City Name                      | [الفيوم ]\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import weather\n",
    "input_text = \"ما احوال الجو بعد يومين فى الفيوم\"\n",
    "text, tokens, ents, tokens_verb_noun = main.NLU(input_text, stopwords, ner_instance, verbs, nouns)\n",
    "content, city_name = weather.main(tokens, tokens_verb_noun,ents)\n",
    "print(\"---------------------------------------------------------------------------------------\")\n",
    "print(\"Input                          |\"+\" \"+ input_text)\n",
    "print(\"----------------------------------------------------------------------------------------\")\n",
    "print(\"Content                        |\"+\" [\"+ content + \" ]\")\n",
    "print(\"City Name                      |\"+\" [\"+ city_name + \" ]\")\n",
    "print(\"----------------------------------------------------------------------------------------\\n\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "09233404dfd1ef6999d0a311c79761d8562782d272c00e285041b29904c81873"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
