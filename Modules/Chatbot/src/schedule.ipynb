{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import tokenization\n",
    "import preprocess\n",
    "import ner\n",
    "import verb_extraction\n",
    "import stemming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['واحد', 'اتنين ', 'تلاته', 'اربعه', 'خمسه', 'سته', 'سبعه', 'تمانيه', 'تسعه', 'عشره', 'واحدعشر', 'اتنين عشر', 'تلاتهعشر', 'اربعهعشر', 'خمسهعشر', 'ستهعشر', 'سبعهعشر', 'تمانيهعشر', 'تسعهعشر', 'عشرين', 'واحدوعشرين', 'اتنين وعشرين', 'تلاتهوعشرين', 'اربعهوعشرين', 'خمسهوعشرين', 'ستهوعشرين', 'سبعهوعشرين', 'تمانيهوعشرين', 'تسعهوعشرين', 'تلاتين', 'واحدوتلاتين', 'اتنين وتلاتين', 'تلاتهوتلاتين', 'اربعهوتلاتين', 'خمسهوتلاتين', 'ستهوتلاتين', 'سبعهوتلاتين', 'تمانيهوتلاتين', 'تسعهوتلاتين', 'اربعين', 'واحدواربعين', 'اتنين واربعين', 'تلاتهواربعين', 'اربعهواربعين', 'خمسهواربعين', 'ستهواربعين', 'سبعهواربعين', 'تمانيهواربعين', 'تسعهواربعين', 'خمسين', 'واحدوخمسين', 'اتنين وخمسين', 'تلاتهوخمسين', 'اربعهوخمسين', 'خمسهوخمسين', 'ستهوخمسين', 'سبعهوخمسين', 'تمانيهوخمسين', 'تسعهوخمسين']\n"
     ]
    }
   ],
   "source": [
    "singleNumbers = [\"واحد\",\"اتنين \",\"تلاته\",\"اربعه\",\"خمسه\",\"سته\",\"سبعه\",\"تمانيه\",\"تسعه\"]\n",
    "higherNumbers = [\"عشر\",\"عشرين\",\"تلاتين\",\"اربعين\",\"خمسين\"]\n",
    "numbersBagOfWords = []\n",
    "for number in singleNumbers:\n",
    "    numbersBagOfWords.append(number)    \n",
    "\n",
    "for higher in higherNumbers:\n",
    "    if higher == \"عشر\":\n",
    "        numbersBagOfWords.append(higher + \"ه\")\n",
    "    else:\n",
    "        numbersBagOfWords.append(higher)\n",
    "    for number in singleNumbers:\n",
    "        if higher == \"عشر\" or higher == \"\":\n",
    "            numbersBagOfWords.append(number + higher)    \n",
    "        else:\n",
    "            numbersBagOfWords.append(number + \"و\" + higher)\n",
    "print(numbersBagOfWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit_distance (word1, word2, index1, index2):\n",
    "    dp = [[0 for x in range(index2 + 1)] for x in range(index1 + 1)]\n",
    "    for i in range (index1 + 1):\n",
    "        for j in range (index2 + 1):\n",
    "            if i == 0:\n",
    "                dp[i][j] == j\n",
    "            elif j == 0:\n",
    "                dp[i][j] == i\n",
    "            elif word1[i - 1] == word2[j - 1]:\n",
    "                dp[i][j] = dp[i-1][j-1]\n",
    "            else:\n",
    "                 dp[i][j] = 1 + min ( dp[i-1][j-1], #replace\n",
    "                                    dp[i-1][j],     #remove\n",
    "                                    dp[i][j-1])     #insert\n",
    "    return dp[index1][index2]\n",
    "    \n",
    "def textNumberToNumber(word, numbersBagOfWords):\n",
    "    if word.isnumeric():\n",
    "        return int(word), 0\n",
    "    editDistance = 1000\n",
    "    closestNumber = \"واحد\"\n",
    "    for index, number in enumerate(numbersBagOfWords):\n",
    "        editDistanceNew = edit_distance(word, number, len(word), len(number))\n",
    "        if (editDistanceNew < editDistance or \n",
    "        (editDistanceNew == editDistance and abs( len(number) - len(word)) < abs( len(closestTextualNumber) - len(word)))):\n",
    "            editDistance = editDistanceNew\n",
    "            closestTextualNumber = number\n",
    "            closestNumber = index + 1\n",
    "    if editDistance >= 3:\n",
    "        return 0 , editDistance\n",
    "    return closestNumber, editDistance\n",
    "    \n",
    "def bigrams (tokens):\n",
    "    grams = []\n",
    "    for index in range(len(tokens) - 1):\n",
    "        grams.append([tokens[index] , tokens[index + 1]])\n",
    "    return grams\n",
    "\n",
    "def get_closest_word (word, tokens):\n",
    "    min_dist = 10000\n",
    "    min_index = 0\n",
    "    for index, wordInText in enumerate(tokens):\n",
    "        distance = edit_distance(word,wordInText, len(word), len(wordInText))\n",
    "        if distance < min_dist:\n",
    "            min_dist = distance\n",
    "            min_index = index\n",
    "    return min_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "اظبطلي المنبه 4 الفجر بكره\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ahmed Sherif\\anaconda3\\lib\\site-packages\\sklearn\\base.py:310: UserWarning: Trying to unpickle estimator CountVectorizer from version 0.23.1 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ahmed Sherif\\anaconda3\\lib\\site-packages\\sklearn\\base.py:310: UserWarning: Trying to unpickle estimator LinearSVC from version 0.23.1 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ahmed Sherif\\anaconda3\\lib\\site-packages\\sklearn\\base.py:310: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.23.1 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def NLU(text):\n",
    "    #Preprocessing\n",
    "    text = preprocess.pre_process(text)\n",
    "    #Tokenization\n",
    "    tokens = tokenization.get_tokens(text)\n",
    "    # NER\n",
    "    ents = ner.get_ents(tokens)\n",
    "    #Part of Speech and Stemming\n",
    "    tokens_verb_noun = verb_extraction.extract_stem_verb(tokens,ents)\n",
    "    tokens_verb_noun = stemming.stem(tokens_verb_noun)\n",
    "    return text , tokens ,ents , tokens_verb_noun\n",
    "text = input()\n",
    "text, tokens, ents, tokens_verb_noun = NLU(text)\n",
    "\n",
    "filtered_tokens = []\n",
    "for index, token in enumerate(tokens):\n",
    "    if tokens_verb_noun[index][1] != \"vOrder\":\n",
    "        filtered_tokens.append(token)\n",
    "\n",
    "for index, token in enumerate(filtered_tokens):\n",
    "    if token[0] == 'ا' and token[1] == 'ل':\n",
    "        filtered_tokens[index] = token[2:]\n",
    "    elif token[0] == 'و' :\n",
    "        filtered_tokens[index] = token[1:]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 4, 1, 0, 1, 0, 0, 0] [0, 0, 1, 1, 1] ['اظبطلي', 'منبه', '4', 'فجر', 'بكره']\n",
      "اظبطلي منبه\n",
      "اظبطلي المنبه\n"
     ]
    }
   ],
   "source": [
    "bigram = bigrams(filtered_tokens)\n",
    "bigram.reverse()\n",
    "# minuteEdit, hourEdit, dayEdit, monthEdit, overwriteHour, overwriteDay, overwriteMonth, at night\n",
    "edits = [0, 0, 0, 0, 0, 0, 0, 1]\n",
    "# specifier -> [check index in edits, how much to add]\n",
    "time_specifiers_second = {\n",
    "    \"ساعه\" : [1, 1], \"يوم\" : [2, 1], \"شهر\" : [3, 1], \"بكره\" : [2, 1],\n",
    "    \"ساعتين\" : [1, 2], \"يومين\" : [2, 2], \"شهرين\" : [3, 2],\n",
    "    \"ساعات\" : [1, 3], \"ايام\" : [2, 3], \"شهور\" : [3, 3],\n",
    "    \"ربع\" : [0, 15], \"تلت\" : [0, 20], \"نص\" : [0, 30],\n",
    "    \"دقايق\" : [0, 3], \"دقيقه\" : [0, 3]\n",
    "}\n",
    "time_specifiers_first = {\n",
    "    \"ساعه\" : [1], \"يوم\" : [2], \"شهر\" : [3]\n",
    "}\n",
    "tokens_used = [0] * len(filtered_tokens)\n",
    "for index, gram in enumerate(bigram):\n",
    "    if gram[0] in time_specifiers_first :\n",
    "        specifier = time_specifiers_first.get(gram[0])\n",
    "        if edits[specifier[0]] == 0:\n",
    "            edits[specifier[0]], _ = textNumberToNumber(gram[1], numbersBagOfWords)\n",
    "            edits[specifier[0] + 3] = 1\n",
    "            tokens_used[filtered_tokens.index(gram[0])] = 1\n",
    "            tokens_used[filtered_tokens.index(gram[1])] = 1\n",
    "    \n",
    "    if gram[1] in time_specifiers_second:\n",
    "        specifier = time_specifiers_second.get(gram[1])\n",
    "        if edits[specifier[0]] == 0:\n",
    "            edits[specifier[0]] = specifier[1]\n",
    "            if specifier[1] == 3:\n",
    "                edits[specifier[0]], _ = textNumberToNumber(gram[0], numbersBagOfWords) \n",
    "                tokens_used[filtered_tokens.index(gram[0])] = 1\n",
    "            tokens_used[filtered_tokens.index(gram[1])] = 1\n",
    "\n",
    "\n",
    "# no specifiers\n",
    "if edits[1] == 0:            \n",
    "    minDistance = 1000\n",
    "    token_index = -1\n",
    "    for index, token in enumerate(filtered_tokens):\n",
    "        if tokens_used[index] == 0:\n",
    "            hoursEdit, distance = textNumberToNumber(token, numbersBagOfWords)\n",
    "            if hoursEdit != 0 and distance < minDistance:\n",
    "                minDistance = distance\n",
    "                edits[1] = hoursEdit\n",
    "                edits[4] = 1\n",
    "                token_index = index\n",
    "    if token_index != -1:\n",
    "        tokens_used[token_index] = 1\n",
    "\n",
    "time_specifiers_tokens = {\n",
    "    \"صبح\" : [0], \"ليل\" : [1], \"صباح\" : [0], \"نهار\" : [1], \"ظهر\" : [1], \"عصر\" : [1], \"مغرب\" : [1] ,\n",
    "    \"عشاء\" : [1], \"عشا\" : [1], \"فجر\" : [0]\n",
    "}\n",
    "for token in filtered_tokens:\n",
    "    if token in time_specifiers_tokens:\n",
    "        tokens_used[filtered_tokens.index(token)] = 1\n",
    "        edits[7] = time_specifiers_tokens.get(token)[0]\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "print (edits, tokens_used, filtered_tokens)\n",
    "\n",
    "longest_subsequence, longest_subsequence_start, start_index, last_index = -1, 0, 0, 0\n",
    "for index, used_token in enumerate(tokens_used):\n",
    "    if used_token == 0:\n",
    "        last_index += 1\n",
    "    else:\n",
    "        if last_index - start_index > longest_subsequence :\n",
    "            longest_subsequence = last_index - start_index\n",
    "            longest_subsequence_start = start_index\n",
    "        start_index, last_index =  index + 1, index + 1\n",
    "if used_token == 0:\n",
    "    if last_index - start_index > longest_subsequence :\n",
    "            longest_subsequence = last_index - start_index\n",
    "            longest_subsequence_start = start_index\n",
    "        \n",
    "filtered_tokens[longest_subsequence_start]\n",
    "print (filtered_tokens[longest_subsequence_start], filtered_tokens[ longest_subsequence_start + longest_subsequence - 1])\n",
    "\n",
    "text_splitted = text.split()\n",
    "content = text_splitted[ get_closest_word (filtered_tokens[longest_subsequence_start], text_splitted):\n",
    " get_closest_word (filtered_tokens[ longest_subsequence_start + longest_subsequence - 1], text_splitted) + 1]\n",
    "content = \" \".join(content)\n",
    "print(content)\n",
    "# need to add \"بكرة\" \"الصبح\" \"بليل\" \n",
    "# cannot deiffrentiate between \"watch\" and \"hour\" both are \"ساعة\"\n",
    "# it is a static code any error will result into a distaster XD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-07-03 18:17:23.107417\n",
      "2022-07-03 04:00:00\n",
      "2022-07-04 04:00:00\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "now_time = datetime.now()\n",
    "print(now_time)\n",
    "new_time = datetime(year= now_time.year\n",
    "                    , month= ( edits[3] * edits[6] ) + ( now_time.month * (not edits[6]) )\n",
    "                    , day=   ( edits[2] * edits[5] ) + ( now_time.day * (not edits[5]) )\n",
    "                    , hour=  ( edits[1] * edits[4] + 12 * edits[7]) + ( now_time.hour * (not edits[4]) )\n",
    "                    , minute=( edits[0] * edits[4] ) + ( now_time.minute * (not edits[4]) ))\n",
    "print(new_time)\n",
    "new_time = new_time + relativedelta(minutes= edits[0] * (not edits[4]), hours=edits[1] * (not edits[4]), days=edits[2] * (not edits[5]), months=edits[3] * (not edits[6]))\n",
    "print(new_time)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4416bc9836c12781b1e5ff1e266afc9f29e38369dcfeb27a2853c274cac55a02"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
